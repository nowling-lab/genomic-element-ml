#!/usr/bin/env python3

import argparse
from collections import defaultdict
from collections import OrderedDict
import sys

import numpy as np

from sklearn.ensemble import BaggingClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder

def read_fasta(flname):
    # since Python 3.7, LIFO is guaranteed
    # but in case this is run on an older
    # version of Python...
    sequences = OrderedDict()
    with open(flname) as fl:
        seq_name = None
        seq = ""
        for ln in fl:
            if ln.startswith(">"):
                if seq_name != None:
                    sequences[seq_name] = seq
                seq_name = ln[1:].strip().split()[0]
                seq = ""
            else:
                seq += ln.strip()

        if seq_name != None:
            sequences[seq_name] = seq

    return sequences

def write_bed(flname, seq_names, labels, seq_probs):
    with open(flname, "w") as fl:
        for seq_name, label, prob in zip(seq_names, labels, seq_probs):
            chrom, remaining = seq_name.split(":")
            start, end = remaining.split("-")
            fl.write(chrom)
            fl.write("\t")
            fl.write(start)
            fl.write("\t")
            fl.write(end)
            fl.write("\t")
            fl.write(seq_name)
            fl.write("\t")
            fl.write(str(label))
            fl.write("\t")
            fl.write(str(prob))
            fl.write("\n")

def main():
    parser = argparse.ArgumentParser()
    
    parser.add_argument("--training-tmnt-fasta",
                        required=True,
                        type=str,
                        help="Treatment training sequences")

    parser.add_argument("--training-ctrl-fasta",
                        required=True,
                        type=str,
                        help="Control training sequences")

    parser.add_argument("--target-tmnt-fasta",
                        required=True,
                        type=str,
                        help="Treatment target sequences")

    parser.add_argument("--target-ctrl-fasta",
                        required=True,
                        type=str,
                        help="Control target sequences")
    
    parser.add_argument("--reg-weight",
                        required=True,
                        type=float,
                        help="L2 regularization weight")

    parser.add_argument("--n-jobs",
                        default=1,
                        type=int,
                        help="Use multiple threads")

    parser.add_argument("--n-estimators",
                        type=int,
                        help="Number of models to use in ensemble")

    parser.add_argument("--output-bed-fl",
                        type=str,
                        required=True)

    args = parser.parse_args()

    training_tmnt_seq = read_fasta(args.training_tmnt_fasta)
    training_ctrl_seq = read_fasta(args.training_ctrl_fasta)
    target_tmnt_seq = read_fasta(args.target_tmnt_fasta)
    target_ctrl_seq = read_fasta(args.target_ctrl_fasta)

    print("Vectorizing")
    all_training_seq = []
    all_training_labels = []
    all_training_seq.extend(training_tmnt_seq.values())
    all_training_labels.extend([1.] * len(training_tmnt_seq.values()))
    all_training_seq.extend(training_ctrl_seq.values())
    all_training_labels.extend([0.] * len(training_ctrl_seq.values()))

    all_target_seq = []
    all_target_labels = []
    all_target_keys = []
    all_target_keys.extend(target_tmnt_seq.keys())
    all_target_keys.extend(target_ctrl_seq.keys())
    all_target_seq.extend(target_tmnt_seq.values())
    all_target_labels.extend([1.] * len(target_tmnt_seq.values()))
    all_target_seq.extend(target_ctrl_seq.values())
    all_target_labels.extend([0.] * len(target_ctrl_seq.values()))
        
    vec = CountVectorizer(ngram_range=(6, 8),
                          analyzer="char")
    training_X = vec.fit_transform(all_training_seq)

    print("Training model")
    sgd = SGDClassifier(loss="log_loss",
                        alpha=args.reg_weight)
    # Bootstrapping didn't help with reducing variance or accuracy
    # Goal is to compensate for variance from random
    # weight initialization of SGD classifier
    ensemble = BaggingClassifier(sgd,
                                 n_estimators = args.n_estimators,
                                 bootstrap = False,
                                 n_jobs=args.n_jobs)    

    ensemble.fit(training_X, all_training_labels)
    


    print("Classifying")
    target_X = vec.transform(all_target_seq)
    pred_prob = ensemble.predict_proba(target_X)[:, 1]

    write_bed(args.output_bed_fl,
              all_target_keys,
              all_target_labels,
              pred_prob)

    roc_auc = roc_auc_score(all_target_labels, pred_prob)
    print("ROC AUC: {:.1%}".format(roc_auc))
    
if __name__ == "__main__":
    main()
